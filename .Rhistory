library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
remove.packages("ggplot2")
remove.packages("GGally")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
-3--5
library(bbmle)
library(ggplot2)
library(plyr)
priorPreg <- c (2, 7, 5, 0, 0, 3, 0, 4, 10)
censored  <- c (0, 0, 0, 0, 0, 0, 0, 0, 1 )
# p <- 1/mean(priorPreg + 1)
data <- data.frame(priorPreg, censored)
preg.in.month.log.L <- function(p, data, negll=FALSE) { # PUT PARAMETER AS FIRST ARGUMENT
# Negative Binomial independent sampling [do not do coding yourself]
expOne <- sum( (data[,2]==0)*(data[,1] * log(1-p) + log(p)))
expTwo <- sum( (data[,2]==1)*(data[,1]* log(1-p)))
res <- expOne + expTwo
if(negll){res <- -res}
return(res)
}
preg.in.month.log.L (.2, data, negll=TRUE)
nrow(data)
str(data)
plotdata<-NULL
## 3 Use a function from the plyr package to compute the log-likelihood between
## 0 and 0.5 in steps of .01 and plot the log-likelihood function
plotdata<-data.frame(p<-seq(0, 0.5, 0.01))
str(plotdata)
plotdata$logL <- laply(plotdata$p, preg.in.month.log.L, data=data)
head(plotdata$logL)
pguess <- 0.15
mymle       <- optim(pguess, preg.in.month.log.L, data=data, negll=TRUE, hessian=TRUE)
mymle
se <- sqrt(1/mymle$hessian)
se
preg.in.month.log.L (0.2051017, data)
plot <- ggplot(data=plotdata, aes(x=p, y=logL))+
geom_line()
plot.mle <- ggplot(data=plotdata, aes(x=p, y=logL))+
ggtitle("Maximum likelihood of conceiving through optimization")+
geom_line()+
geom_point(aes(x=mymle$par, y=-mymle$value), size=4, shape=2)+
scale_shape_identity()
plot.mle
## Use the nlm() function to find the MLE
nlm.preg <- nlm(preg.in.month.log.L, pguess, data = data, negll=TRUE, hessian=TRUE)
nlm.preg
se.nlm  <- sqrt(1/nlm.preg$hessian)
mle.nlm <- nlm.preg$estimate
mle.nlm.Y <- preg.in.month.log.L (mle.nlm, data)
#Getting the Ys coresponding to the log of LCL and UCL
ucl.nlm <- mle.nlm + qnorm(0.975) * se.nlm
ucl.nlm.Y <- preg.in.month.log.L (ucl.nlm, data)
lcl.nlm <- mle.nlm - qnorm(0.975) * se.nlm
lcl.nlm.Y <- preg.in.month.log.L (lcl.nlm, data)
## 5 Add the MLE to the previous plot by plotting a filled circle at the maximum
## of the likelihood and the returned MLE.
plot.nlm <- plot + geom_point(data=plotdata, mapping=aes(x=mle.nlm, y= mle.nlm.Y, colour="red",shape=21),size=4)+
geom_point(data=plotdata, mapping=aes(x= ucl.nlm, y = ucl.nlm.Y, color ="red",shape=21),size=4)+
geom_point(data=plotdata, mapping=aes(x=lcl.nlm, y = lcl.nlm.Y, color ="red",shape=21),size=4) +
scale_shape_identity()
plot.nlm.anno <- plot.nlm +annotate('text', x = c(0.05,0.2,0.4), y = c(-22,-18.5,-20), label=c("LCL","MLE","UCL"),
size= c(4,4,4),colour="red")+
annotate('text', x = c(0.05,0.2,0.4), y = c(-20.5,-17,-18.8), label=round(c(lcl.nlm, mle.nlm, ucl.nlm), digits=2),
size= c(4,4,4),colour="red")
plot.nlm.anno
## 9 Load the bbmle package and repeat the optimization using the mle2()
mle2.preg <- mle2(preg.in.month.log.L, list(p = pguess ), data= list(data = data, negll=TRUE))
mle.bbl.preg <- coef(mle2.preg)
mle.bbl.Y <- preg.in.month.log.L (mle.bbl.preg, data)
CI <- confint(mle2.preg)
ucl.mle2.Y <- preg.in.month.log.L (CI[2], data)
lcl.mle2.Y <- preg.in.month.log.L (CI[1], data)
se.mle2 <- sqrt(diag(vcov(mle2.preg)))
plot.mle2 <- plot.nlm.anno + geom_point(data=plotdata, mapping=aes(x=coef(mle2.preg), y= mle.bbl.Y, colour="blue", shape=21),size=2)+
geom_point(data=plotdata, mapping=aes(x= confint(mle2.preg)[2], y = ucl.mle2.Y, color ="blue", shape=21), size=4)+
geom_point(data=plotdata, mapping=aes(x= confint(mle2.preg)[1], y = lcl.mle2.Y, color ="blue", shape=21), size=4)+
ggtitle("MLE of conceiving with 95% \nCI using NLM and BBMLE ")
plot.mle2
#
#
#
#    ZAPNBUOY DATA
#    Read in teh zap data that was preprocessed in Matlab to strip unncessary
#    perform basic analysis:
#    dsddl09=zap1209(zap1209.site_id_==21309,:);
#    dsddl09=table2dataset(dsddl09);
#    newzap=[dsddl09.buoy_douglas dsddl09.avgall__top_sum dsddl09.avg__win1m_stddev_diff2_top_sum dsddl09.stddev__diff2_top_sum];
#    csvwrite('newzap.csv',newzap)
#    Supervisor: Dr. Mehrdad Oveisi
#    Saida Amirova 301086263
#    April install20th, 2015
#   – analyze the results from an experiment with a categorical response
#   – estimate population marginal parameters
#     plot these items on a suitable plots.
#
################################################
################################################
##             My Function                    ##
################################################
mysummary <- function(mydf, var, alpha=0.05){
# Compute the number of element, number of non-missing elements
# mean, sd, and se of mydf$var
# mydf is assumed to be a Dataframe and values are collected using a CRD/SRS
values <-mydf[,var]
ntotal <- length(values)
nonmiss <- length(na.omit(values))
mean <- mean(values)
sd <- sd(values)
se <- sd/sqrt(nonmiss)
ha<-1-alpha/2
error<-qt(ha,nonmiss-1)*se
lcl <- mean-error
ucl<- mean+error
return(data.frame(ntotal,nonmiss,mean,sd,se,lcl,ucl))
}
########################################################
########################################################
#rm(list=ls())
setwd("/Users/Saida/Desktop/VIVA")
library(car)
library(plyr)
library(ggplot2)
library(plotly)
library(scales)
require(foreign)
require(ggplot2)
require(MASS)
require(Hmisc)
require(reshape2)
zap <- read.csv("newzap.csv", header=FALSE,strip.white=TRUE,as.is=TRUE)
# Heading first 5 rows of DDL09 site
zap[1:5,]
# Renaming columns using plyr
names(zap)
zap <- rename(zap,c("V1"="douglas", "V2"="topavgsum","V3"="stddev","V4"="stddevdiff","V5"="height"))
zap[1:5,]
xtabs(~douglas,data=zap)
# Create douglas scale as a factor
zap$douglasC  <- recode(zap$douglas,"1='One'; 2='Two'; 3='Three'; 4='Four'")
zap$douglasC  <- factor(zap$douglasC,levels=c("One","Two","Three","Four"),
ordered=TRUE)
str(zap$douglasC)
# Log transform topavg sum
zap$logA <- log(-zap$topavgsum)
str(zap$logA)
# Creating boxplot using the ggplot2
boxavg <- ggplot(zap,aes(x=douglasC,y=topavgsum))+
ggtitle("Notched boxplot of \n Avg. TopSum vs Douglas scale")+
geom_boxplot(aes(fill = douglasC),notch=TRUE,notchwidth = 0.5, outlier.size=2, outlier.shape=2)+
geom_point(aes(color = douglasC),size=0.5)+
geom_jitter(colour=alpha("black",0.1) )+
scale_x_discrete("Douglas Scale",labels=c("1","2","3","4"))+
scale_y_continuous("Avg.TopSum \n outliers marked as triangle")+
scale_color_manual(breaks = c("1","2","3","4"),
labels = c("1","2","3","4"),
values = c("#E69F00","#56B4E9","#009E73","#F0E442"))+
scale_fill_manual(breaks = c("1","2","3","4"),
labels = c("1","2","3","4"),
values = c("#E69F00","#56B4E9","#009E73","#F0E442"))
#coord_flip()
boxavg
ggsave("boxavg.png", width=6, height=4, dpi=400)
transformavg <- ggplot(zap,aes(x=douglasC,y=logA), color=douglasC)+
ggtitle("Notched boxplot of \n Avg. TopSum vs Douglas scale")+
geom_boxplot(notch=TRUE,notchwidth = 0.5, outlier.size=0.2, outlier.shape=2)+
geom_point(size=0.5)+
geom_jitter(colour=alpha("black",0.05) )+
scale_x_discrete("Douglas Scale",labels=c("1","2","3","4"))+
scale_y_continuous("Avg.TopSum")+
geom_hex( bins=50 )
transformavg
douglas.sum <- ddply(zap, "douglasC", mysummary, "topavgsum", alpha=0.01)
douglas.sum
#Plotting summary of the mean scatter large CI in 4th category due to small sample size
plot.mean.douglasC <- ggplot(data=douglas.sum, aes(x=douglasC, y=mean))+
ggtitle("Mean Scatter by state")+
xlab("Douglas Scale")+ylab("Sound scatter (avg top sum)")+
geom_point(size=1,shape=2)+geom_line(aes(group=1))+geom_point(size=3)+
geom_errorbar(aes(ymin=lcl,ymax=ucl),width=0.15)+
scale_colour_manual(name="Error Bars",values=c("lcl","ucl"))+
theme(panel.background = element_rect(fill='light blue', colour='black'))+
annotate('text', x = c(1,2.5,3.5,3.5), y = c(-5,-7,-10,-20), label=c("-6.27 (-6.57,-5.97)","-7.96 (-8.14. -7.77)","-11.46 (-11.81, -11.12)","-19.11 (-22.62, -15.60)"),
size= c(4,4,4,4),colour="black")
plot.mean.douglasC
ggsave("plot.mean.douglasC.png", width=6, height=4, dpi=400)
#rm(plot.mean.douglasC)
## Looking at distribution of data which is clearly non normal and has a heavy negative skew
qqnorm(zap$topavgsum);qqline(zap$topavgsum, col = 2)
#Intercept only model
m <- polr(douglasC ~ topavgsum + stddevdiff + stddev, data = zap, Hess=TRUE)
summary(m)
(ctable <- coef(summary(m)))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p))
(ci <- confint(m)) # default method gives profiled CIs
confint.default(m) # CIs assuming normality
## PREPARING MACHINE LEARNING UNSUPERVISED
row.has.na <- apply(zap, 1, function(x){any(is.na(x))})
sum(row.has.na)
zap.filter<- zap[!row.has.na,]
#library(rpart)
#zap.ml <- zap[,2:4]
# fit model
#fit.cart <- rpart(douglasC~., data=zap)
# summarize the fit
#summary(fit.cart)
# make predictions
#zap[1,2:4]
#predictions <- predict(fit.cart, type="class")
# summarize accuracy
#table(predictions, zap$douglasC)
# Validation
#n      <- nrow(zap)
#K      <- 10
#taille <- n%/%K
#set.seed(5)
#alea   <- runif(n)
#rang   <- rank(alea)
#bloc   <- (rang-1)%/%taille + 1
#bloc   <- as.factor(bloc)
#print (summary(bloc))
#all.err <- numeric(0)
#print(summary(bloc))
#for (k in 1:K){
#
#         tree <- rpart(douglasC~.,data=zap[bloc!=k,],method="class")
#         pred <- predict(tree,newdata=zap[bloc==k,],type="class")
#         mc<- table(zap$douglasC[bloc==k],pred)
#         err<-1-(mc[1,]+mc[2,2])/sum(mc)
#         all.err<-rbind(all.err,err)
#}
#print(all.err)
#err.cv <- mean(all.err[,1])
#py <- plotly()
#py$ggplotly(boxavg)
## SUEPRVISED MACHINE LEARNING
library(class)
library(caret)
zap.filter$douglasC <- NULL
train.X <- zap.filter[,-2]
test.X  <- zap.filter[,-2]
train.Y <- zap.filter[,1]
test.Y  <- zap.filter[,1]
knn.fit  <- knn(train.X,test.X,train.Y,k=3,prob=TRUE)
table    <- table(knn.fit,test.Y)
mat.conf <- confusionMatrix(table)
#tr.roc <- roc(zap.filter$douglas, zap.filter$topavgsum,percent=TRUE,
#              # arguments for auc
#              partial.auc=c(90,100), partial.auc.correct=TRUE,
#              partial.auc.focus="sens",
#              # arguments for ci
#              ci=TRUE, boot.n=5, ci.alpha=0.9, stratified=FALSE,
#              # arguments for plot
#              plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
#              print.auc=TRUE, show.thres=TRUE)
#tr.roc
mat.conf
clear
setwd("/Users/Saida/Desktop/CANDISP/")
candisp <- read.csv('dispfromkml.csv',header=TRUE,strip.white=TRUE,as.is=TRUE)
candisp[3,]
points <- cbind(candisp$lon, candisp$lat)
str(points)
pts   <- CRS("+proj=longlat +ellps=WGS84")
sp_pt <- SpatialPoints(points, proj4string = pts)
summary(sp_pt)
library(sp)
library(maps)
library(maptools)  #shapefiles
library(ggmap)
library(rgdal)
library(scales)    #transperancies
